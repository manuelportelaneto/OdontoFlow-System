"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var N8nLlmTracing_exports = {};
__export(N8nLlmTracing_exports, {
  N8nLlmTracing: () => N8nLlmTracing
});
module.exports = __toCommonJS(N8nLlmTracing_exports);
var import_base = require("@langchain/core/callbacks/base");
var import_base2 = require("@langchain/core/language_models/base");
var import_tiktoken = require("@langchain/core/utils/tiktoken");
var import_pick = __toESM(require("lodash/pick"));
var import_n8n_workflow = require("n8n-workflow");
var import_helpers = require("../../utils/helpers");
const TIKTOKEN_ESTIMATE_MODEL = "gpt-4o";
class N8nLlmTracing extends import_base.BaseCallbackHandler {
  constructor(executionFunctions, options) {
    super();
    this.executionFunctions = executionFunctions;
    this.name = "N8nLlmTracing";
    // This flag makes sure that LangChain will wait for the handlers to finish before continuing
    // This is crucial for the handleLLMError handler to work correctly (it should be called before the error is propagated to the root node)
    this.awaitHandlers = true;
    this.connectionType = import_n8n_workflow.NodeConnectionTypes.AiLanguageModel;
    this.promptTokensEstimate = 0;
    this.completionTokensEstimate = 0;
    /**
     * A map to associate LLM run IDs to run details.
     * Key: Unique identifier for each LLM run (run ID)
     * Value: RunDetails object
     *
     */
    this.runsMap = {};
    this.options = {
      // Default(OpenAI format) parser
      tokensUsageParser: (llmOutput) => {
        const completionTokens = llmOutput?.tokenUsage?.completionTokens ?? 0;
        const promptTokens = llmOutput?.tokenUsage?.promptTokens ?? 0;
        return {
          completionTokens,
          promptTokens,
          totalTokens: completionTokens + promptTokens
        };
      },
      errorDescriptionMapper: (error) => error.description
    };
    this.options = { ...this.options, ...options };
  }
  async estimateTokensFromGeneration(generations) {
    const messages = generations.flatMap((gen) => gen.map((g) => g.text));
    return await this.estimateTokensFromStringList(messages);
  }
  async estimateTokensFromStringList(list) {
    const embeddingModel = (0, import_base2.getModelNameForTiktoken)(TIKTOKEN_ESTIMATE_MODEL);
    const encoder = await (0, import_tiktoken.encodingForModel)(embeddingModel);
    const encodedListLength = await Promise.all(
      list.map(async (text) => encoder.encode(text).length)
    );
    return encodedListLength.reduce((acc, curr) => acc + curr, 0);
  }
  async handleLLMEnd(output, runId) {
    const runDetails = this.runsMap[runId] ?? { index: Object.keys(this.runsMap).length };
    output.generations = output.generations.map(
      (gen) => gen.map((g) => (0, import_pick.default)(g, ["text", "generationInfo"]))
    );
    const tokenUsageEstimate = {
      completionTokens: 0,
      promptTokens: 0,
      totalTokens: 0
    };
    const tokenUsage = this.options.tokensUsageParser(output.llmOutput);
    if (output.generations.length > 0) {
      tokenUsageEstimate.completionTokens = await this.estimateTokensFromGeneration(
        output.generations
      );
      tokenUsageEstimate.promptTokens = this.promptTokensEstimate;
      tokenUsageEstimate.totalTokens = tokenUsageEstimate.completionTokens + this.promptTokensEstimate;
    }
    const response = {
      response: { generations: output.generations }
    };
    if (tokenUsage.completionTokens > 0) {
      response.tokenUsage = tokenUsage;
    } else {
      response.tokenUsageEstimate = tokenUsageEstimate;
    }
    const parsedMessages = typeof runDetails.messages === "string" ? runDetails.messages : runDetails.messages.map((message) => {
      if (typeof message === "string") return message;
      if (typeof message?.toJSON === "function") return message.toJSON();
      return message;
    });
    this.executionFunctions.addOutputData(this.connectionType, runDetails.index, [
      [{ json: { ...response } }]
    ]);
    (0, import_helpers.logAiEvent)(this.executionFunctions, "ai-llm-generated-output", {
      messages: parsedMessages,
      options: runDetails.options,
      response
    });
  }
  async handleLLMStart(llm, prompts, runId) {
    const estimatedTokens = await this.estimateTokensFromStringList(prompts);
    const options = llm.type === "constructor" ? llm.kwargs : llm;
    const { index } = this.executionFunctions.addInputData(this.connectionType, [
      [
        {
          json: {
            messages: prompts,
            estimatedTokens,
            options
          }
        }
      ]
    ]);
    this.runsMap[runId] = {
      index,
      options,
      messages: prompts
    };
    this.promptTokensEstimate = estimatedTokens;
  }
  async handleLLMError(error, runId, parentRunId) {
    const runDetails = this.runsMap[runId] ?? { index: Object.keys(this.runsMap).length };
    if (typeof error === "object" && error?.hasOwnProperty("headers")) {
      const errorWithHeaders = error;
      Object.keys(errorWithHeaders.headers).forEach((key) => {
        if (!key.startsWith("x-")) {
          delete errorWithHeaders.headers[key];
        }
      });
    }
    if (error instanceof import_n8n_workflow.NodeError) {
      if (this.options.errorDescriptionMapper) {
        error.description = this.options.errorDescriptionMapper(error);
      }
      this.executionFunctions.addOutputData(this.connectionType, runDetails.index, error);
    } else {
      this.executionFunctions.addOutputData(
        this.connectionType,
        runDetails.index,
        new import_n8n_workflow.NodeOperationError(this.executionFunctions.getNode(), error, {
          functionality: "configuration-node"
        })
      );
    }
    (0, import_helpers.logAiEvent)(this.executionFunctions, "ai-llm-errored", {
      error: Object.keys(error).length === 0 ? error.toString() : error,
      runId,
      parentRunId
    });
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  N8nLlmTracing
});
//# sourceMappingURL=N8nLlmTracing.js.map